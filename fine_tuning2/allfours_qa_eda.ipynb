{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c812cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acca956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Queue', 'Question', 'Answer', 'Reasoning', 'Chapter', 'Type'],\n",
       "    num_rows: 598\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'reasoned_qa_output/all_reasoned_qa.json'\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af425f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simplified Split Results ---\n",
      "Training set total: 478\n",
      "Validation set total: 60\n",
      "Test set total: 60\n",
      "\n",
      "Splitting complete!\n"
     ]
    }
   ],
   "source": [
    "all_data = list(dataset)\n",
    "\n",
    "# 1. Create the labels for stratification (by Chapter only)\n",
    "stratify_labels = [str(d['Chapter']) for d in all_data]\n",
    "\n",
    "# 2. First split: Get the 80% training set.\n",
    "#    The other 20% is a temporary set for validation and testing.\n",
    "train_data, test_data, train_label, test_label = train_test_split(\n",
    "    all_data,\n",
    "    stratify_labels,\n",
    "    test_size=0.1,      # Splitting off 20%\n",
    "    random_state=614,\n",
    "    stratify=stratify_labels\n",
    ")\n",
    "\n",
    "# 3. Second split: Split the 20% temporary set in half.\n",
    "#    This gives 10% for validation and 10% for testing.\n",
    "train_data, val_data, _, _ = train_test_split(\n",
    "    train_data,\n",
    "    train_label,\n",
    "    test_size=1/9,\n",
    "    random_state=614,\n",
    "    stratify=train_label\n",
    ")\n",
    "\n",
    "print(\"--- Simplified Split Results ---\")\n",
    "print(f\"Training set total: {len(train_data)}\")\n",
    "print(f\"Validation set total: {len(val_data)}\")\n",
    "print(f\"Test set total: {len(test_data)}\")\n",
    "print(\"\\nSplitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7361354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# --- Re-create the count dictionaries using ONLY 'Chapter' as the key ---\n",
    "\n",
    "train_counts = defaultdict(int)\n",
    "val_counts = defaultdict(int)\n",
    "test_counts = defaultdict(int)\n",
    "\n",
    "# Count items by Chapter in the training set\n",
    "for item in train_data:\n",
    "    key = item[\"Chapter\"]  # Use 'Chapter' only\n",
    "    train_counts[key] += 1\n",
    "\n",
    "# Count items by Chapter in the validation set\n",
    "for item in val_data:\n",
    "    key = item[\"Chapter\"]  # Use 'Chapter' only\n",
    "    val_counts[key] += 1\n",
    "\n",
    "# Count items by Chapter in the test set\n",
    "for item in test_data:\n",
    "    key = item[\"Chapter\"]  # Use 'Chapter' only\n",
    "    test_counts[key] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6d51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distribution of Chapters Across Datasets ---\n",
      "Total Sizes -> Train: 478, Validation: 60, Test: 60\n",
      "\n",
      "    Chapter Train %  Val % Test % Raw Counts (Tr/V/T)\n",
      "0         1   3.35%  3.33%  3.33%              16/2/2\n",
      "1         2   3.35%  3.33%  3.33%              16/2/2\n",
      "2         3   3.35%  3.33%  3.33%              16/2/2\n",
      "3         4   3.35%  3.33%  3.33%              16/2/2\n",
      "4         5   3.14%  3.33%  3.33%              15/2/2\n",
      "5         6   3.35%  3.33%  3.33%              16/2/2\n",
      "6         7   3.35%  3.33%  3.33%              16/2/2\n",
      "7         8   3.14%  3.33%  3.33%              15/2/2\n",
      "8         9   3.35%  3.33%  3.33%              16/2/2\n",
      "9        10   3.35%  3.33%  3.33%              16/2/2\n",
      "10       11   3.35%  3.33%  3.33%              16/2/2\n",
      "11       12   3.35%  3.33%  3.33%              16/2/2\n",
      "12       13   3.35%  3.33%  3.33%              16/2/2\n",
      "13       14   3.35%  3.33%  3.33%              16/2/2\n",
      "14       15   3.35%  3.33%  3.33%              16/2/2\n",
      "15       16   3.35%  3.33%  3.33%              16/2/2\n",
      "16       17   3.35%  3.33%  3.33%              16/2/2\n",
      "17       18   3.35%  3.33%  3.33%              16/2/2\n",
      "18       19   3.35%  3.33%  3.33%              16/2/2\n",
      "19       20   3.35%  3.33%  3.33%              16/2/2\n",
      "20       21   3.35%  3.33%  3.33%              16/2/2\n",
      "21       22   3.35%  3.33%  3.33%              16/2/2\n",
      "22       23   3.35%  3.33%  3.33%              16/2/2\n",
      "23       24   3.35%  3.33%  3.33%              16/2/2\n",
      "24       25   3.35%  3.33%  3.33%              16/2/2\n",
      "25       26   3.35%  3.33%  3.33%              16/2/2\n",
      "26       27   3.35%  3.33%  3.33%              16/2/2\n",
      "27       28   3.35%  3.33%  3.33%              16/2/2\n",
      "28       29   3.35%  3.33%  3.33%              16/2/2\n",
      "29       30   3.35%  3.33%  3.33%              16/2/2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This code is the same as before, but now it uses the correctly populated dictionaries\n",
    "\n",
    "# 1. Get the total size of each dataset\n",
    "total_train = len(train_data)\n",
    "total_val = len(val_data)\n",
    "total_test = len(test_data)\n",
    "\n",
    "# 2. Get a sorted list of all unique chapters\n",
    "all_chapters = sorted(set(train_counts.keys()) | set(val_counts.keys()) | set(test_counts.keys()))\n",
    "\n",
    "# 3. Prepare the data for display\n",
    "results_list = []\n",
    "for chapter in all_chapters:\n",
    "    train_c = train_counts.get(chapter, 0)\n",
    "    val_c = val_counts.get(chapter, 0)\n",
    "    test_c = test_counts.get(chapter, 0)\n",
    "    \n",
    "    train_pct = (train_c / total_train) * 100 if total_train > 0 else 0\n",
    "    val_pct = (val_c / total_val) * 100 if total_val > 0 else 0\n",
    "    test_pct = (test_c / total_test) * 100 if total_test > 0 else 0\n",
    "    \n",
    "    results_list.append({\n",
    "        \"Chapter\": chapter,\n",
    "        \"Train %\": f\"{train_pct:.2f}%\",\n",
    "        \"Val %\": f\"{val_pct:.2f}%\",\n",
    "        \"Test %\": f\"{test_pct:.2f}%\",\n",
    "        \"Raw Counts (Tr/V/T)\": f\"{train_c}/{val_c}/{test_c}\"\n",
    "    })\n",
    "\n",
    "# 4. Create and print the DataFrame\n",
    "df_distribution = pd.DataFrame(results_list)\n",
    "print(f\"\\n--- Distribution of Chapters Across Datasets ---\")\n",
    "print(f\"Total Sizes -> Train: {total_train}, Validation: {total_val}, Test: {total_test}\\n\")\n",
    "print(df_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec1748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved train_data to allfours_train_data.json!\n",
      "Successfully saved val_data to allfours_val_data.json!\n",
      "Successfully saved test_data to allfours_test_data.json!\n"
     ]
    }
   ],
   "source": [
    "# to data frame\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_val = pd.DataFrame(val_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# choose Q&A&R for fine tuning\n",
    "df_train = df_train[['Question', 'Answer', 'Reasoning']]\n",
    "df_val = df_val[['Question', 'Answer', 'Reasoning']]\n",
    "df_test = df_test[['Question', 'Answer', 'Reasoning']]\n",
    "\n",
    "# stroe to json files\n",
    "df_train.to_json(\"reasoned_qa_output/allfours_train_data.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_val.to_json(\"reasoned_qa_output/allfours_val_data.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_test.to_json(\"reasoned_qa_output/allfours_test_data.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "\n",
    "print(\"Successfully saved train_data to allfours_train_data.json!\")\n",
    "print(\"Successfully saved val_data to allfours_val_data.json!\")\n",
    "print(\"Successfully saved test_data to allfours_test_data.json!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892e2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209ccfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa99ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
