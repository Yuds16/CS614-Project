{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99408bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute as needed\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e8296",
   "metadata": {},
   "source": [
    "## Process Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53aca0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "RAW_BOOKS_FOLDER_PATH = \"../raw_data/books/\"\n",
    "PROCESSED_BOOKS_FOLDER_PATH = \"../processed_data/books/\"\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4284d530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .DS_Store\n",
      ".txt\n",
      "Tracker file is empty or corrupted.\n",
      "Skipping .DS_Store as it is not a valid file.\n",
      "Processing Sunrise on the Reaping.pdf\n",
      "Sunrise on the Reaping.txt\n",
      "Tracker file is empty or corrupted.\n",
      "Tracker file is empty or corrupted.\n",
      "Finished processing Sunrise on the Reaping.pdf.\n",
      "Processing All Fours (Miranda July).pdf\n",
      "All Fours (Miranda July).txt\n",
      "Finished processing All Fours (Miranda July).pdf.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "from modules.embedding_tracking import add_new_file, file_exists\n",
    "\n",
    "available_files = os.listdir(RAW_BOOKS_FOLDER_PATH)\n",
    "\n",
    "for i in available_files:\n",
    "    print(f\"Processing {i}\")\n",
    "    \n",
    "    processed_fname = \".\".join(i.split(\".\")[:-1])+\".txt\"\n",
    "    print(processed_fname)\n",
    "    \n",
    "    if file_exists(processed_fname):\n",
    "        print(f\"Skipping {i} as it has already been processed.\")\n",
    "        continue\n",
    "\n",
    "    text = \"\"\n",
    "    if i.endswith(\".pdf\"):\n",
    "        fname = os.path.join(RAW_BOOKS_FOLDER_PATH, i)\n",
    "        pdf_document = fitz.open(fname)\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            page_text = page.get_text()\n",
    "            # Fix spaces between words\n",
    "            page_text = \" \".join(page_text.replace(\"\\n\", \" \").split())\n",
    "            text += page_text + \"\\n\"\n",
    "        pdf_document.close()\n",
    "            \n",
    "    else:\n",
    "        # invalid file types\n",
    "        print(f\"Skipping {i} as it is not a valid file.\")\n",
    "        continue\n",
    "    \n",
    "    with open(os.path.join(PROCESSED_BOOKS_FOLDER_PATH, processed_fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "        add_new_file(processed_fname)\n",
    "        \n",
    "    print(f\"Finished processing {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3deab",
   "metadata": {},
   "source": [
    "### Store to storage as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0967e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yudhistiraonggowarsito/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing All Fours (Miranda July).txt\n",
      "Skipping All Fours (Miranda July).txt for nomic-embed-text as it has already been processed with suffix 512_chunks.\n",
      "Skipping All Fours (Miranda July).txt for mxbai-embed-large as it has already been processed with suffix 512_chunks.\n",
      "Inserted data with IDs: ['64370857-e9ca-4a39-b3d1-aaceb8671478', '47f4f4c0-673f-421c-9b9a-6dff377dbeb4', 'cc377a3b-d292-45db-b1d0-ea4fc862710e', 'ec035d1a-1e0b-4ced-82b2-f0717e253ad6', '9529b113-0292-454a-b3a3-b85cbbe948a3', '24f0a8d1-7629-4cb9-8817-a451c99510b7', '20b86b3c-d348-4a07-83ed-c3a654bf4e1f', '15b753f8-b91a-4b2f-869b-7802b8e39510', '9e65c589-ff4a-4da1-a25c-6e66d32c4244', '907af3c0-c8dd-43b9-8fe1-a1880f9ec6de', '2f9615c7-ff12-43a1-9645-c2a5c53d35cf', 'eeca1e3d-97c4-4cbd-a8e1-5ebd46f7a3bd', 'e98a8eab-5bf5-4af9-97f4-eb930f94f78a', '0b727c0e-f222-416d-9b58-04f8b2b485c1', 'aafcde2b-ce4f-43c9-80dd-58dab92d8727', '6ca526cf-a5ca-4ba0-bd59-1419c07db402', 'c55c492d-0338-40bb-9cf1-0a72604094ff', 'f1ef2090-d95f-4542-8f5f-4cb52a4173ee', '69eea183-8046-4a14-bf65-fec324fd2ec9', '833716ff-2879-4435-b2e0-c60117f40061', '991b7913-c790-4953-86e1-d5bcc4d13d10', 'd6fe763d-6488-4a0d-a2f6-8dc3f2874d3b', '6e9bcb7d-a510-4cd1-96ec-dd03e89c060c', 'ced705cc-e5f7-4bcf-8018-532683877b2a', '169275d8-1835-4379-8580-aefe85f31ec8', 'fe2581c4-8231-4348-94b5-ac3cff0a4767', 'c75b2666-1ccc-4d9e-b2c1-396b8c72fa4d', 'eb54af7d-a158-442e-91a6-b7083187ebe5', '33f4a36a-ba15-4e5d-83a8-eb2d1c4c381b', '36496254-c21e-496a-8750-64c03b5f39db', 'e38c9c2f-1a5c-4180-a639-635b601bb24a', '53962811-fcc1-42f4-9f55-6b51a7dbc56c', 'eb11c245-65db-4905-bf1d-9cf30acc2825', '1b416f08-6f5d-46fa-a18f-a6b19ffdfcdf', 'e7165077-3073-49dc-97a2-05e7d0e5436a', 'c7557a32-ec23-4fdd-b676-5a910fa078e8', '8010f457-804e-4670-84a8-6de2b4b0d3f5', '9ca5bb81-5b34-4e5b-9f9d-6fad705065a5', 'c3c2da5a-df60-4aaa-8d02-3b8cb78f2ac4', '1f59c685-1bc6-44a7-9df1-8ae187b34e09', '3394643a-0495-483f-8a11-b4d6991884db', '62246090-43ce-486d-8d5d-11da1266667a', '98144c34-4ab9-4cc6-81b5-4605b40fed9e', '4a4ecc14-74b4-4408-b7ce-b7b8c7c155ba', '1def0f61-3e59-4ffd-a8a1-fa6c3d2a6782', 'e8430e59-d9c9-4098-b728-873ee4c73502', '6996047d-ac4d-424f-8321-927c5ea1f3d4', '266e038b-f334-40db-9a98-11bece38d435', '4a1f7c01-42fd-4b5c-9cac-5aa4d40f6e4e', '15aabcde-f528-4acf-a8d9-39145cfd3568', '3df24847-119e-460c-b552-186625ed8fff', '574f9eb9-d354-4eb8-a29c-eca857037bdc', '3b0410dc-f195-4043-86ec-c5104c63e47c', 'f4faa7e1-bc18-4714-befc-72afceeb76bb', 'cea6c4ef-47c5-4f40-8274-9a3b0fd61527', '47b9d24b-3caf-4598-972c-a410a7e4b61c', 'aa5f8f64-1336-4ef0-841d-b3a61dbef5c8', 'a55e2488-5070-4d91-80ff-43c166d32474', '48cb7185-9087-4f62-813b-5ba9fabdb63c', 'c6b823c9-7241-45d5-aeb9-3bd89cca5a8f', 'dcbbe29e-bf3e-4888-81f9-3cb57372fd12', '50f3a225-c15d-4b71-8ea0-623e388eb1c6', 'a4a972f3-8582-4eec-a445-bddd483ef58a', 'ce312367-b930-442d-91f2-4bd42b5fbd2a', 'f62cc7e2-beaa-422a-9306-ab7cce8a4c4a', 'ed985c73-105c-4abc-bb3c-9e6e7e843bdd', '1857e377-9ca8-4228-a045-5c52c200f334', '3ac8493d-c584-495f-90ec-ffb95922e0ed', '08b3f5a5-80d5-485d-b2ac-99df8e779570', 'b3ffb2c1-63f2-4e67-9a32-8b96634cd509', '18fa5797-08f5-4a83-ac67-5875bbc1fae6', 'c7bc9a57-e018-4fcd-a599-877ddb0b3236', '7e763f37-3d3d-4c42-bea2-21fac16f6acd', '640c15a4-3b1b-452a-8663-7e821e06e1e5', 'e95a4e81-2968-46d5-a17a-5531942d0969', 'afb57bdf-363c-4cc0-94b1-d25b46b37f22', '95c025ea-324b-4c94-bb0f-b290dca63ebc', '9a36a314-8925-45d1-926a-1e3ac62e1d52', '8d4c716a-a9f1-4abc-8f6c-c5c112443976', 'be0215c5-c088-44e2-91c6-1d66f5882dd9', 'dd8bfcaf-dfe8-4815-a9cf-906123b25bd3', '00143572-3301-41d2-ac74-d2df262f730b', '22ec66af-6de8-4367-a237-a71e6bf35818', '68797f31-be5d-4035-9e0f-72d8770d403a', '6293df30-5927-4666-ba2c-c1440d170a34', 'fb92efd0-e824-43fb-a129-43c04a5cefa0', '44863bfc-c867-4959-8a46-7210d1a4dbe8', '87f8fb7c-86e7-4943-a39d-0a1de3b2dd2a', 'ba7931ec-ad6c-462d-9965-be2bba1465d4', '6e8edd77-2e1f-4ee2-8719-0c5748bb0793', '0e999012-96e2-4bbe-aa84-f22f4f090169', 'd43e3399-a955-43be-a963-f1510c3131b2', 'ae496902-dd28-4b1d-8bb0-b4d91db090f5', 'dcb2c8a1-2d61-4d99-bdd5-ad0f73200ca8', '3b2bcd16-16df-4e3e-9c9f-5631b2281673', '9fc2689a-8fbd-40c4-bff7-6baf0c2bc1e6', '28aa96c0-0a3d-42c1-8974-efbeaf82e6e5', '4356dbf3-6b54-4e75-941e-255742ebe4cb', '3b579f4e-8fdf-491b-b761-e2c2ace4595a', 'af25a8be-4a53-4fad-8be3-a98c78d4402b', 'cd5b7e0a-2db4-4820-a3c3-cd35b7dc68ef', '2852875f-759d-4aac-b651-31e76390bb26', '93353dca-33cd-4845-9973-08e78b5484f1', 'bad4805e-74fc-4ae6-b11e-7ede9a87b241', '41678958-82ea-47de-a07d-d1b75b2dc7b3', 'bfe76015-9a1e-400e-8fb6-31dba07774a2', '44cc4e09-163b-4131-a05a-d015df0f13c4', '74e45fcf-e520-46cf-aeed-3dbf7ac92779', 'b5c94364-0365-4a23-93a8-df1fafb4c044', '201690c7-1534-414e-b7ad-59ca15221e3c', 'f1f00cdc-583d-4ef6-8a0e-1a2435057ae0', 'dbb58e83-16d0-4094-a37f-2c54b444db02', '6d6bb6a1-119d-46ef-995c-0acc3fa05479', '6ca19c34-c451-4b65-9ec7-ab76e405b111', '6b1a1fcb-625f-41b6-a1d0-95f841f9dee7', '00ecb96d-75e2-4960-982a-e70142bb70e8', '1867c5ac-a4c5-45ae-9de8-c31b2d13386b', '6d4704b5-3b9a-40bf-b319-61b85f8de6d5', '82547bfd-8e37-45a2-a5b2-d0966bc993ff', '22b85f89-675d-45ab-a28e-d0b0d2777566', '95c1d764-a5df-448d-87a7-5801e41d3e1a', '1d02d5c5-fc74-4fc7-be42-13b2319601f8', 'b309a2b6-65ff-4b1b-bac2-4af4d968e936', 'e055847d-fb24-468f-a48e-c95d86c5cac2', 'fcb783e6-0ad3-49fb-a237-d6bcfdafaaa8', '874e5ee8-9557-4415-9e37-6630ef4207da', '2fd0e081-37a2-440b-9d6f-c0b71c5d2b91', 'abcd1853-ba81-45b4-96fa-368984fb5c64', '1abbb28c-93f8-4ea8-bba0-13e4632719af', '2eea8f06-213d-47f3-9ca1-c7d6c06d8da2', '95c23ac9-3498-4de4-968c-cabfe8600666', '650490b4-73e8-45ea-bce5-853d2de1a732', 'b5b7a425-b84c-4a0c-99db-0df092a72cec', '9542faca-81fe-471c-bd46-16c1695169d4', '1b5562f5-21a7-448c-bdec-80d739b4044b', '8e93694f-46f8-4de3-a8b2-b3c9ade216a4', 'e10b6006-16c2-4076-a4ab-4faf25f52de6', 'f8978a53-200c-4644-92d8-a13d951eb141', '2be624c8-c997-45fe-afb5-87437b090372', 'ddffe46e-e278-4362-99d0-8c3c2fef147f', 'b2df6949-3685-4c3c-93cd-5df874c7f694', '12c85473-3b9f-48c3-b459-a9c31c25daf0', '0f22ee55-6ab9-4edc-b2a0-de89a04afe03', '48eb5213-3614-45ad-8cbe-7bca4e3a4d00', '6d001eb2-6f97-420e-9a11-e83cbe62c284', 'f0fc2e47-7045-4dfe-88e5-c8b068a092fe', '960e96f5-2441-44fb-a37d-88d1353faa68', '6548e771-adb7-44ce-bab0-aee9f4af2198', 'bedbc4a6-181f-47fa-a961-e89eed6e7191', '9fb6ef82-61fc-4a3f-b5c4-4942027f2ce7', '288ca11c-d4f4-429a-8c9e-a6f61a311475', '235d2131-6a70-4e17-98d3-01fb21d09d4e', 'e2223b8c-a265-4997-9680-10743cebdcc9', '5d13fc95-8616-473d-85f6-5e68f717ad94', '9726f297-a285-408a-a1bf-761fffa7e50b', 'a7e3f2db-c039-4bba-97a4-e59fa7f72c56', '29b8f596-f727-4538-88ff-84eba95a0a38', 'ad819748-b62b-4b62-a5f4-f49a8ab455d5', '30c25b95-6174-4a3a-b67c-a5ca508ea4d2', '4bd47935-5c0a-4cf4-b1e3-9dca8f42fea9', 'e6459069-b708-4f81-97f9-d911c30a088b', '0fe10324-806f-4525-a2fc-2c153ebf7223', 'ab29ace4-312b-434f-bc55-9db842b4dfb8', '62f16383-79cc-458d-9d7b-9b550b0de809', 'ebf2a3ae-e240-4b08-bf2d-1b063ad74c67', '66441ee8-c7c9-4387-9ec9-2549f74b2872', '4f90d00b-419d-41fc-8090-76748965c06c', '1e6d1c09-d3f2-44ae-b2bb-4147b86bba04', '6a079282-5b1a-4e17-9946-781a746cacb1', '90634ff4-db8d-4e26-9b77-b246887a47ad', '1bc8f482-e604-4259-b2bd-eb8ff4e839db', 'e9254c45-cc4b-4428-af0e-48f22d91615a', '11094c7f-3d6a-4dad-81d0-7610f53e1c3a', '27d388c7-a61f-44b9-98d6-c31379ecf541', '997edd0e-82df-427c-9fb3-f7c44ad2a71e', '5041d892-d7d9-4c45-9e76-60bb764f6e60', 'a1ab3c0d-1491-4564-a00b-6e3ba4739afe', 'ec14d7ea-5949-4a75-807e-9c344ae66740', '71ad6e09-c535-46fc-acc4-c6cf0e243dea', '01cc3e9d-77df-4d09-8c41-4b3563fac35e', 'cceaa485-ddfe-43db-8fd2-9bb2521a690c', 'e62185fb-03d4-427d-8a9b-cb0c3398e29c', '15496685-3273-47e7-9921-5ee12de75799', '98613986-efd8-47dc-9e5d-751ea7f8488c', '13ff9b5b-4f4f-46d1-9c08-258a6ca9e693', 'd67c8c9f-08a0-46ec-be58-7951c6257601', 'dc11a1c6-b240-425e-bf79-a8b6557bda6f', 'b61b6218-5aad-4a0a-9837-7a7cc342fe1d', '1363db16-5191-422c-96b5-c2b527f83dca', '67addd5c-5f9f-4242-80fd-2b39b3dca7fd', '0a026fad-af9f-4313-839b-25e3f0494e42', 'ba982311-39ad-40ae-b9fe-5af54c1fd7d0', '047ab2d9-a51e-465c-9032-24f36a2b0fc6', '6312aa4f-b53d-428d-87de-6211cef12ae1', '54eedcee-8db5-457a-8bb6-1d40b0671038', '1ef964e3-f9db-46d5-98f7-8689350944e0', '7525da7f-a2ce-4906-b983-8ceedf784564', '298ad7e5-34eb-437a-a3df-d1a6f7b044fb', 'aa3e73d5-3aab-4bb1-b063-6d67a7ca05bf', '61325fb4-a0f8-4cfb-b024-e9871a5553c7', '9275789b-446a-4ab5-a73c-5b02b2d73e3f', '8cf030ed-d6ce-4b44-96e3-8d729eaba41b', 'c21026ce-56f4-44e7-b1b3-909156ea9b9e', 'b1ac5f28-ab20-4267-8629-cb53ce4e04bc', '288a091e-88cb-491a-bc08-e6c87eee3b4a', '96f638fb-d16e-4b55-a66f-ffd68494a3e3', '6e75e85a-5e62-4947-99ff-f8ba5ed643fe', '6d2cabf1-44a7-4207-be2a-fca8860e977a', '5684405c-75ff-4dc9-9c16-aee70bada132', '8dae3156-4e0c-462b-b24f-a666b65097bf', 'a1aa439e-841b-4518-990b-35e3f9a6f48e', 'ea269279-c3bf-4764-953d-d21f1e9f0e02', '19a47ffd-2b7a-4d36-a887-96a55cc971bb', '05467583-3b08-43bb-ae3b-bc21d041e6b6', '5755ca96-0daa-495f-9422-f99e0349e07f', '6b052cb6-f383-4d96-8e4c-43895142d72e', '62f5808c-4ce6-4a4c-b8cd-5a8d404482c0', '8fefe734-718b-4cb5-a9aa-3f11a5d4a1f7', '0ddd21af-c135-4c67-9a48-a60e51b453c2', '75bd80b1-eaa1-468a-bd0a-85163ef41b8e', 'feeccf01-5f3e-4ef0-aa58-a6712d4c9469', '0aa23a8a-f106-49ad-9c5f-2e81e843a634', '5d509b70-f00c-4a8e-8ff7-14856405bcd3', '86b458c1-ee55-45bf-b971-034154ecabfd', '73d31cb9-68b4-462d-84f5-ee435c94f116', 'c82692c6-29b9-4c5b-b2e4-d650ef090628', '14272c87-77b8-449b-9aeb-ace80e86d34a', '3717127a-076f-4f36-b0ff-afdeb8a2538e', 'c7513052-7eec-41d9-98cb-68a8c21a8681', '4293330c-d689-4c37-a663-6f0234a10017', '22bcae60-daa0-41ec-9683-dbc613b25846', 'b04adef0-978e-483e-b67e-e42f962c67de', 'ff942191-a942-4af3-9ffc-a6da9f9c5989', '24daf519-8d72-43cb-9d49-e0c322157a83', '9b90a8bd-1578-488f-8d98-3d467feee312', 'b011d499-487e-43c8-bb98-5dbf911e5177', '72fd82a3-2ecd-4a2f-9009-b14f42436f19', '32e1aa03-21e1-4756-afdb-952f196b36e2', '172bfafb-eb5b-4ccf-bc7f-31acbb5d31ac', '1ae20249-d193-4766-801c-8a549ef16143', '35a3fab8-61ec-42f4-bd8f-0ee815cf8b54', '394754ad-340c-4640-b90d-366013454e72', 'bc319c81-82a1-4f10-a3ce-634649deb566', '767a15f6-ef5a-46c8-9721-b5233cbc1c3f', '44c5baf9-5b13-4704-9b0a-a3cbb1742974', '9f7005ea-6b85-4dd6-bfff-b0c4cdf364e9', '58610a23-3db6-43d0-90e6-49f76d9a8233', '9da45f29-3dcd-493d-811f-08f8dfe3cb29', '4a271d02-b9c4-411b-9e6a-917a9694fcfb', '9fadfe35-c894-4c78-8a6b-cf68801680c6', 'd563a858-aee0-4855-ad85-ba76cf24167f', 'f39a1a85-9fba-406b-aa63-d306a9d4c57d', '53e69fb1-718e-4bdf-b678-06cc739a1ef3', '3e6b7f99-8cfd-49c4-bfa6-78e2718aa55c', '54fe3ebf-6240-47a8-84ae-e705cf64cdc1', '79d00ac7-8a22-42b4-a86f-ac050d69e16a', '5353ac7e-a8c5-471b-b0c3-feb99483ec44', '0f85f266-8c7f-4f75-8160-f68ce018f9f7', '204ce5e3-5de5-43a9-b236-e41f504eb4c9', '227dcbcf-8c70-49a9-8310-32c39dee7f63', 'cc94dc5c-1910-4956-a63d-5d45f7566c3c', '85d5101e-5ab2-4850-9d8b-ec0230af2c87', '393063a3-c871-491a-8952-59bb26b1896b', 'a9e593d2-55fe-440f-8d77-da3838ff3e64', '03347f11-a99e-4285-b71d-b6e2daa0a6eb', 'a28a293e-d95e-4834-8092-a6dddb8e9ad4', '1db0bdcb-93e1-459b-a919-65d62c5e19b4', '4bd0f7e4-0ebd-41ad-8101-c2aa1df2cb8c', 'afc5c2da-a1a6-45d7-889e-d6c9bade1988', '85b8e091-35c5-4da8-ab52-e8783717bf27', 'eb3ebb8c-4006-470f-b699-69b15de93baf', '97480d01-ba39-4e52-9658-a7db87bc262f', '7431fe59-1072-4527-8d51-06828d60deb5', '53d4a89c-b632-4605-89b2-1bf4f5e38f23', 'ca404835-411d-4d04-8a71-3f39e2fd955c', '0d59b98f-b066-44df-9fbd-904f9f32da6d', '0b2f91b4-5fc3-4a76-9a8f-b8c0c12e1710', '3b6c8a99-95c9-43e6-b426-cc7247660a4f', '6e853504-972a-4be0-8f18-858441f975d3', '800d0154-45b1-4a8f-8f5a-a1d03bd47ffe', 'fe705f27-019b-473c-a244-642e8120f3e5', '5ea5deac-d400-45e7-9018-93bab2f87e4f', 'c21b73bf-1f4b-442e-bd37-677a5c4ea3f9', '88ce7a39-efe4-41f0-b7b1-7e81c92fcc19', 'aa422de9-121f-452e-9e9a-ff77994be5c4', 'd841d8c4-f4d9-4f4c-b5fe-7ae467fe6581', '9c77497e-a7e2-48a4-a61c-77e79b1e94ab', '6fc563a4-dc53-4054-b19a-6632bb5cbd74', '1a8edb75-bd1a-4792-95e0-f9483fe1637b', '79aa707b-552b-4656-92c3-82ff4a62c1a0', 'fc0eb4db-3eee-4a3d-86aa-1f69949ac896', 'f04926f8-3974-47c3-8b48-14d0f9dd35cd', 'b7930dd7-2150-4e4d-9153-ae950c5b6deb', '47226d2f-aab4-4b15-b482-b350eb10fe63', '20fe47df-2b72-4c09-ac31-90e3deff4eb2', 'ba41b267-0ee3-4f2a-a855-bd73a37519b1', 'dcfacfd2-0b5e-4dff-8518-efcc65ef8fdc', '437ef86b-7fb2-4495-a575-360eb0fa1ce6', '74c64fbe-7470-4ef9-a4d6-450b842c446f', '9d17233a-ad06-4742-adc9-060b9187fc0d', '1aa789d8-f176-4ddc-8f47-3f1eb4420827', '74bc00d0-c65c-4b51-a089-9ca840366026', 'ad823e71-3eab-4039-86e6-a0ac7b90fdc6', '09607176-03f9-41af-b439-23e60af92657', 'a79cff56-d809-4902-8b35-f671a5bc871a', '9dc5c957-06b5-4a51-b165-568f09fa5c9a', 'da43d0d6-082c-40f8-a18e-2c5f91799e19', 'a85474d3-11dd-4ac5-ba0c-58eeb616bca1', '82cf4533-652a-4467-abc2-9faf3c8d31be', '6d650951-d1e9-498c-a517-78355c843f3d', 'e18ff5ba-198d-4b2f-8927-bdcd9d42a38b', 'dd24b16c-c3b4-48bb-a16b-fe6eb56fbe5a', 'c0d12ba9-232d-4b12-afb9-59b83eaa6985', '717aa509-f5b7-4ab2-a155-aeb0dcfbf594', '956e28a8-6417-4bdf-906f-962af141995e', '8e6d3f93-4bf7-471d-b50b-17b2606e336d', '31c246f6-3a95-4579-92ee-4754fc35fd2a', 'edaf291e-7c46-449e-a2cd-f64521580781', '4d89405b-2206-4560-bf3d-8d94b57e3461', '51de2b98-ada9-4edc-ae3a-0e5ae4d97f3b', '431024fc-d208-4671-9c31-2425e087ff69', 'eb3d7bdd-2d4f-4d43-86fe-3f3d0bb6d006', '80d4a289-a712-4aca-8357-bb33552e7d87', '51f1dfde-147c-494f-aff0-fe37d053020b', '661293ae-a01e-4747-9cf6-bc50e76e3e64', '4f65ebfd-500d-49ca-b221-a7d17aeab055', '2a7a612b-b392-451f-a96a-e22fe1410dc0', '1eb1e5ef-7ecd-4778-9d5b-9c06a403a638', 'e86f0e92-1b6e-414b-acc5-b05ee1a0370a', '6baa6660-7503-4c34-94fc-c5c7bc04ca53', 'e790355a-cb13-47e3-95c2-3af5372f47e1', '20d7e7c1-ae5f-4328-a3b3-5101d0d5abca', 'c068b34e-ebe3-4afe-926f-59ec4f030cde', '4adc9e00-869f-4cbe-85b7-6b5a770f541c', '501fd834-0074-4aec-92f4-773346ff32ce', '6a572221-476b-4fe6-ab2e-a6ba43030625', '0ad8ca20-04c4-4450-8f19-f0bdc70a90ec', 'a6c9acbc-e566-435a-be35-d5f278bce1c7', 'eaaee703-5e6f-4d59-b203-cc2a1dda1919', '171b67ce-b9cf-49f5-ad5a-09771f45b213', 'ec6e26e6-a444-479d-8d44-bca39e2cc660', '5ef03d01-5e92-4653-b66e-c51be328445c', '7c0533ac-ecb9-43fb-84f2-5973463a1e94', '3132bb66-8c75-4b67-8d28-e362a6a23911', '9ecc8f0c-d651-45e1-b96e-7de3cb61bd98'] into collection 'mxbai-embed-large_collection_semantic_chunks'.\n",
      "Skipping All Fours (Miranda July).txt for bge-m3 as it has already been processed with suffix 512_chunks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences)-\u001b[32m1\u001b[39m):\n\u001b[32m     89\u001b[39m     embeddings_1 = accessor.EMBEDDING_FUNCTIONS.get(embedding_function).embed_query(sentences[i])\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     embeddings_2 = \u001b[43maccessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEMBEDDING_FUNCTIONS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     emb_1 = np.array(embeddings_1)\n\u001b[32m     93\u001b[39m     emb_2 = np.array(embeddings_2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/langchain_ollama/embeddings.py:272\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed query text.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/langchain_ollama/embeddings.py:265\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     embedded_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/ollama/_client.py:367\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    360\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    361\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    366\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/ollama/_client.py:120\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    119\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     r.raise_for_status()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SMU/Courses/CS614 - Generative AI with Large Language Models/Project/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# https://medium.com/the-ai-forum/semantic-chunking-for-rag-f4733025d5f5\n",
    "\n",
    "from modules import accessor, embedding_tracking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "filenames = os.listdir(PROCESSED_BOOKS_FOLDER_PATH)\n",
    "embedding_functions = list(accessor.EMBEDDING_FUNCTIONS.keys())\n",
    "stitch_title = \"TITLE: {}\\n\"\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(PROCESSED_BOOKS_FOLDER_PATH, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        print(f\"Tokenizing {filename}\")\n",
    "        \n",
    "        for embedding_function in embedding_functions:\n",
    "            # Fixed size chunk text embedding\n",
    "            suffix = \"512_chunks\"\n",
    "            if not embedding_tracking.is_file_processed(filename, \"{}_{}\".format(embedding_function, suffix)):\n",
    "                chunked_texts = [stitch_title.format(filename) + text[i:i+512] for i in range(0, len(text), 512)]\n",
    "                try:\n",
    "                    ids = accessor.insert(\n",
    "                        data=chunked_texts,\n",
    "                        embedding_func=embedding_function,\n",
    "                        custom_suffix=suffix,\n",
    "                    )\n",
    "                    embedding_tracking.mark_file_processed(filename, \"{}_{}\".format(embedding_function, suffix))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing {suffix} for {filename}: {e}\")\n",
    "            else:\n",
    "                print(f\"Skipping {filename} for {embedding_function} as it has already been processed with suffix {suffix}.\")\n",
    "                \n",
    "            # Recursive character text splitting\n",
    "            suffix = \"recursive_chunks\"\n",
    "            if not embedding_tracking.is_file_processed(filename, \"{}_{}\".format(embedding_function, suffix)):\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=512,\n",
    "                    chunk_overlap=100,\n",
    "                    length_function=len,\n",
    "                )\n",
    "                chunks = text_splitter.split_text(text)\n",
    "                chunks = [stitch_title.format(filename) + chunk for chunk in chunks]\n",
    "                try:\n",
    "                    ids = accessor.insert(\n",
    "                        data=chunks,\n",
    "                        embedding_func=embedding_function,\n",
    "                        custom_suffix=suffix,\n",
    "                    )\n",
    "                    embedding_tracking.mark_file_processed(filename, \"{}_{}\".format(embedding_function, suffix))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing {suffix} for {filename}: {e}\")\n",
    "                    \n",
    "            # Separator text splitting\n",
    "            suffix = \"separator_chunks\"\n",
    "            if not embedding_tracking.is_file_processed(filename, \"{}_{}\".format(embedding_function, suffix)):\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=512,\n",
    "                    chunk_overlap=100,\n",
    "                    length_function=len,\n",
    "                    separators=[\"CHAPTER\", \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "                )\n",
    "                chunks = text_splitter.split_text(text)\n",
    "                chunks = [stitch_title.format(filename) + chunk for chunk in chunks]\n",
    "                try:\n",
    "                    ids = accessor.insert(\n",
    "                        data=chunks,\n",
    "                        embedding_func=embedding_function,\n",
    "                        custom_suffix=suffix,\n",
    "                    )\n",
    "                    embedding_tracking.mark_file_processed(filename, \"{}_{}\".format(embedding_function, suffix))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing {suffix} for {filename}: {e}\")\n",
    "            \n",
    "            # Semantic chunking\n",
    "            suffix = \"semantic_chunks\"\n",
    "            if not embedding_tracking.is_file_processed(filename, \"{}_{}\".format(embedding_function, suffix)):\n",
    "                sentences = sent_tokenize(text)\n",
    "                threshold = 0.7 # can be adjusted\n",
    "                chunks = []\n",
    "                curr_chunk = [stitch_title.format(filename), sentences[0]]\n",
    "                for i in range(0, len(sentences)-1):\n",
    "                    \n",
    "                    embeddings_1 = accessor.EMBEDDING_FUNCTIONS.get(embedding_function).embed_query(sentences[i])\n",
    "                    embeddings_2 = accessor.EMBEDDING_FUNCTIONS.get(embedding_function).embed_query(sentences[i+1])\n",
    "                    \n",
    "                    emb_1 = np.array(embeddings_1)\n",
    "                    emb_2 = np.array(embeddings_2)\n",
    "\n",
    "                    if np.linalg.norm(emb_1) == 0 or np.linalg.norm(emb_2) == 0:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        similarity = cosine_similarity([emb_1], [emb_2])[0][0]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating similarity for {sentences[i]} and {sentences[i+1]}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if similarity > threshold:\n",
    "                        chunks.append(curr_chunk)\n",
    "                        curr_chunk = [stitch_title.format(filename), sentences[i+1]]\n",
    "                        continue\n",
    "                    \n",
    "                    curr_chunk.append(sentences[i+1])\n",
    "                \n",
    "                try:\n",
    "                    ids = accessor.insert(\n",
    "                        data=[\" \".join(chunk) for chunk in chunks],\n",
    "                        embedding_func=embedding_function,\n",
    "                        custom_suffix=suffix,\n",
    "                    )\n",
    "                    embedding_tracking.mark_file_processed(filename, \"{}_{}\".format(embedding_function, suffix))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing {suffix} for {filename}: {e}\")\n",
    "            \n",
    "    print(f\"Finished processing {filename} for embeddings.\")\n",
    "    \n",
    "# LLM based chunking\n",
    "suffix = \"llm_chunked\"\n",
    "llm_chunked_fnames = [\"all_fours_txt_chunks.json\", \"sunrise_on_the_reaping_chunks.json\"]\n",
    "for fname in llm_chunked_fnames:\n",
    "    embedding_tracking.add_new_file(fname)\n",
    "    \n",
    "for embedding_function in embedding_functions:\n",
    "    for fname in llm_chunked_fnames:\n",
    "        if not embedding_tracking.is_file_processed(fname, \"{}_{}\".format(embedding_function, suffix)):\n",
    "            try:\n",
    "                with open(os.path.join(\"../llm_chunked_data\", fname), \"r\", encoding=\"utf-8\") as f:\n",
    "                    chunks = json.load(f)\n",
    "                \n",
    "                chunks = [stitch_title.format(fname) + chunk for chunk in chunks]\n",
    "                ids = accessor.insert(\n",
    "                    data=chunks,\n",
    "                    embedding_func=embedding_function,\n",
    "                    custom_suffix=fname,\n",
    "                )\n",
    "                embedding_tracking.mark_file_processed(fname, \"{}_{}\".format(embedding_function, suffix))\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing {suffix} for {fname}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping {fname} for {embedding_function} as it has already been processed with suffix {suffix}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d35fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: ['nomic-embed-text_collection_separator_chunks', 'bge-m3_collection_512_chunks', 'bge-m3_collection_llm_chunked', 'bge-m3_collection_semantic_chunks', 'mxbai-embed-large_collection_semantic_chunks', 'bge-m3_collection_recursive_chunks', 'bge-m3_collection_sunrise_on_the_reaping_chunks.json', 'mxbai-embed-large_collection_llm_chunked', 'mxbai-embed-large_collection_all_fours_txt_chunks.json', 'nomic-embed-text_collection_semantic_chunks', 'mxbai-embed-large_collection_recursive_chunks', 'mxbai-embed-large_collection_512_chunks', 'bge-m3_collection_all_fours_txt_chunks.json', 'nomic-embed-text_collection_sunrise_on_the_reaping_chunks.json', 'nomic-embed-text_collection_all_fours_txt_chunks.json', 'bge-m3_collection_separator_chunks', 'mxbai-embed-large_collection_sunrise_on_the_reaping_chunks.json', 'nomic-embed-text_collection_recursive_chunks', 'mxbai-embed-large_collection_separator_chunks', 'nomic-embed-text_collection_llm_chunks', 'nomic-embed-text_collection_512_chunks', 'langchain']\n",
      "TITLE: Sunrise on the Reaping.txt\n",
      " I will find you. “Haymitch.\n",
      "-------------------------\n",
      "TITLE: Sunrise on the Reaping.txt\n",
      " . Maysilee in the arena . .\n",
      "-------------------------\n",
      "TITLE: Sunrise on the Reaping.txt\n",
      " Not a rainbow. They’re all a deep bloodred. I remember Snow’s rose, his final words to me, and the pieces fall into place. “Spit it out!” I order her, cupping my hand before her mouth. “Spit it out now!” Her face registers shock as she spits a half-chewed gumdrop into my palm. “What? It’s fine.” “Where’s the other? Where’s the first one?” I give her a shake. “I swallowed it, I guess. Why?” “Throw it up! Get it out of your stomach!” She’s panicking now. “What’s going on, Haymitch?” I think of the arena. “Do you all have any charcoal tablets at the house?” “Charcoal tablets? No, I don’t think so. Why would —” I see her put it together. She leans over, sticks a finger down her throat, and tries to gag the thing up. “I can’t do it. I’ve barely eaten in days. There’s nothing to throw up!” “Come on,” I pull her to her feet. “Come on.” I begin to call for help. “Clerk Carmine! Clerk Carmine!” “Haymitch, I —” A perplexed look crosses her face and she presses a hand to her chest. Her knees give way.\n",
      "-------------------------\n",
      "TITLE: Sunrise on the Reaping.txt\n",
      " — and then? Silka dies, her cannon fires, and I’m hanging on by a thread. The sunflower bomb, the quartz, the flint striker — there’s no record of any of them. All of them gone or tucked away from sight. The hovercraft removes Silka’s body. Trumpets declare my victory. A claw closes around me. Are there rules about breaking out of the arena and using the force field to win? Possibly they are implied, but I have never heard them mentioned. So, what am I? A rascal? A cheater even? Maybe. But clearly I do not rise to the standard of a rebel. The camera pulls back slowly as they carry me away, for the first time revealing the arena as a whole. It looks like a giant eye. The Cornucopia marks the pupil. The wide circle of spring-green meadow makes up the iris. On either side, the darker green of the forest and mountain terrain narrows to points, forming the whites of the eye. Well, the symbolism has been lost on no one. Even the little kids in the Seam know the Capitol powers are watching us. I wonder if they ever consider that we’re watching them, too. All eyes on me now, as I rise to my feet before the thundering crowd. The anthem plays as President Snow descends from the heights on a crystal platform, a bloodred rose in his lapel. In his hand, he holds a golden crown. Some victors bow, some kneel, but I just stand there trying to read his expression as he approaches and places the crown on my head. Heavy.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "available_collections = accessor.list_collections()\n",
    "print(f\"Available collections: {available_collections}\")\n",
    "\n",
    "documents = accessor.get(\n",
    "    query=\"In the novel 'Sunrise on the Reaping', what is the final 'poster' Haymitch creates in the arena?\",\n",
    "    embedding_func=\"bge-m3\",\n",
    "    custom_suffix=\"semantic_chunks\",\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "for i in documents:\n",
    "    print(i.page_content)\n",
    "    print(\"-------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
